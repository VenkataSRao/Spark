Before installing Spark, setup sbt and scala..

Below are steps to install sbt,scala and spark 

sbt Installation:
============================

1) Download sbt-0.13.6.tgz 
2) Extract the contents as sbt
3) open ~/.bashrc file.
4) create SBT_HOME environment variable and initialize it as 
    export SBT_HOME=/path/to/sbt
5) save the bashrc file and either restart the terminal or refresh the bashrc file as (source ~/.bashrc)


Scala Installation:
=======================================

1) Download scala-2.10.4.tgz
2) Untar the file and extracts as scala-2.10.4
3) open ~/.bashrc file.
4) create SCALA_HOME environment variable and initialize it as 
    export SCALA_HOME=/path/to/scala-2.10.4
5) save the bashrc file and either restart the terminal or refresh the bashrc file as (source ~/.bashrc)
   
Spark Installation:
==========================

1) Download spark-1.2.1.tgz
2) Untar or extract the contents and this creates the directory spark-1.2.1
3) Make sure that scala and sbt installation is done
4) create SPARK_HOME env variable in bashrc file 
   vim ~/.bashrc
   export SPARK_HOME=/path/to/spark

5) cd spark-1.2.1
   vim conf/spark-env.sh
   Now include SCALA_HOME, HADOOP_HOME, SPARK_MASTER_IP configuration parameters in spark-env.sh. Save and close the file
6) Now open vim conf/slaves. In this file include hostnames of the worker nodes.
7) Once the above steps 4 and 5 are done, change to spark home directory (i.e cd spark-1.2.1)
8) spark-1.2.1$ sbt assembly
   The above command compiles the source code file and downloads necessary dependencies. Access to internet with good speed is required for this.
9) spark-1.2.1$sbt package
   The above command creates jars for all spark subprojects.
10) Once the steps 7 and 8 are done successfully, installation of spark is done.

Start the spark daemons:
spark-1.2.1$./sbin/start-all.sh
This starts the spark daemons master and worker.

