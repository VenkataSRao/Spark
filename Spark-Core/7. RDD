In Spark, work is expressed as either creating new RDD's , transforming existing RDD or calling operation on RDD's to compute 
a result...


RDD: (Resillient Distributed Dataset)
========================================

  a. It is an immutable distributed colletion of object
  b. each RDD split into multiple partitions, which may be computed on different nodes of the cluster
  c. RDD's can can contain any type of java or scala or python object including user defined classes
  d. RDD can be created in 2 ways.. by loading an external dataset or by transforming existing RDD's
