This program demonstrates how to infer schema.. 
for this program below is the sample file..

file name: people.txt
====================
abc,12
sby,39
john,38
jack,27
jon,22
tiger,49

=====================

Code:
==================


package com.spark.demo



import org.apache.spark.sql._
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

case class Person(name: String, age: Int)

object caseclass {
  
  def main (args:Array[String]){
    
    val conf=new SparkConf
    conf.setMaster("local")
    conf.setAppName("caseclass")
    val sc=new SparkContext(conf)
    
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    
    import sqlContext.implicits._
    
    
    
    val people = sc.textFile("/opt/venkata_practice/people.txt").map(_.split(",")).map(p => Person(p(0), p(1).trim.toInt)).toDF()
    
    people.registerTempTable("people")
    
    val teenagers = sqlContext.sql("SELECT name, age FROM people WHERE age >= 18 AND age <= 40")
    
    teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
    
    teenagers.map(t => "Name: " + t.getAs[String]("name")).collect().foreach(println)
    
    teenagers.map(_.getValuesMap[Any](List("name", "age"))).collect().foreach(println)
      
  }

}


Output:
=========
User will get 3 outputs for the above program

1st output:
=========
Name: sby
Name: john
Name: jack
Name: jon

2nd Ouptut:
==========
Name: sby
Name: john
Name: jack
Name: jon

3rd Ouptut:
==========

Map(name -> sby, age -> 39)
Map(name -> john, age -> 38)
Map(name -> jack, age -> 27)
Map(name -> jon, age -> 22)


